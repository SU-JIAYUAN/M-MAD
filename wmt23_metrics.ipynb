{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p30R5QSLqO0R"
   },
   "source": [
    "Colab to reproduce results from the WMT23 metrics shared task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O13agE8mqIJ1"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YvjDTSiEp8zn"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "from mt_metrics_eval import meta_info\n",
    "from mt_metrics_eval import data\n",
    "from mt_metrics_eval import tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FAfC_WDg4sy"
   },
   "source": [
    "# Evaluate a new metric\n",
    "\n",
    "This section shows how to evaluate a new metric online. Another\n",
    "possibility is to generate scores offline, write score files to disk, and use\n",
    "EvalSet.AddMetricsFromDir() to read them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cm1F1I3YVCGI"
   },
   "outputs": [],
   "source": [
    "# @title Load EvalSets\n",
    "\n",
    "wmt23_lps = ['zh-en', 'en-de', 'he-en']\n",
    "evs_dict = {('wmt23', lp): data.EvalSet('wmt23', lp, True) for lp in wmt23_lps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5qRH8Y-iMFH5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('wmt23', 'zh-en'), ('wmt23', 'en-de'), ('wmt23', 'he-en')])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"/home/jiayuansu/M-MAD\")\n",
    "def extract_sys_scores(file_path):\n",
    "    scores = {}\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line by tab character and strip any extra whitespace\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:  # Ensure there are exactly two parts\n",
    "                system_name = parts[0]\n",
    "                score = float(parts[1])  # Convert score to float\n",
    "                scores[system_name] = [score]\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def extract_seg_scores(file_path):\n",
    "    scores = {}\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line by tab character and strip any extra whitespace\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:  # Ensure there are exactly two parts\n",
    "                system_name = parts[0]\n",
    "                score = float(parts[1])  # Convert score to float\n",
    "\n",
    "                # Append the score to the array for the system_name\n",
    "                if system_name in scores:\n",
    "                    scores[system_name] = np.append(scores[system_name], score)\n",
    "                else:\n",
    "                    scores[system_name] = np.array([score])\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "metric_name = 'EAPrompt'\n",
    "\n",
    "for lp in wmt23_lps:\n",
    "  evs = evs_dict[('wmt23', lp)]\n",
    "  sys_scores = extract_sys_scores(f\"EAPrompt_{lp}.sys.score\")\n",
    "  seg_scores = extract_seg_scores(f\"EAPrompt_{lp}.seg.score\")\n",
    "\n",
    "  evs.AddMetric(metric_name, set(), 'sys', sys_scores, replace=True)\n",
    "  evs.AddMetric(metric_name, set(), 'seg', seg_scores, replace=True)\n",
    "\n",
    "\n",
    "for evs in evs_dict.values():\n",
    "  evs.SetPrimaryMetrics(evs.primary_metrics | {metric_name})\n",
    "\n",
    "\n",
    "metric_name = 'GEMBA-DA'\n",
    "\n",
    "for lp in wmt23_lps:\n",
    "  evs = evs_dict[('wmt23', lp)]\n",
    "\n",
    "  sys_scores = extract_sys_scores(f\"GEMBA-DA_{lp}.sys.score\")\n",
    "  seg_scores = extract_seg_scores(f\"GEMBA-DA_{lp}.seg.score\")\n",
    "\n",
    "  evs.AddMetric(metric_name, set(), 'sys', sys_scores, replace=True)\n",
    "  evs.AddMetric(metric_name, set(), 'seg', seg_scores, replace=True)\n",
    "\n",
    "\n",
    "for evs in evs_dict.values():\n",
    "  evs.SetPrimaryMetrics(evs.primary_metrics | {metric_name})\n",
    "\n",
    "\n",
    "metric_name = 'GEMBA-MQM'\n",
    "\n",
    "for lp in wmt23_lps:\n",
    "  evs = evs_dict[('wmt23', lp)]\n",
    "  sys_scores = extract_sys_scores(f\"GEMBA-MQM_{lp}.sys.score\")\n",
    "  seg_scores = extract_seg_scores(f\"GEMBA-MQM_{lp}.seg.score\")\n",
    "\n",
    "  evs.AddMetric(metric_name, set(), 'sys', sys_scores, replace=True)\n",
    "  evs.AddMetric(metric_name, set(), 'seg', seg_scores, replace=True)\n",
    "\n",
    "for evs in evs_dict.values():\n",
    "  evs.SetPrimaryMetrics(evs.primary_metrics | {metric_name})\n",
    "\n",
    "print(evs_dict.keys())\n",
    "\n",
    "\n",
    "metric_name = 'Ours'\n",
    "\n",
    "for lp in wmt23_lps:\n",
    "  evs = evs_dict[('wmt23', lp)]\n",
    "  sys_scores = extract_sys_scores(f\"M-MAD_{lp}.sys.score\")\n",
    "  seg_scores = extract_seg_scores(f\"M-MAD_{lp}.seg.score\")\n",
    "\n",
    "  evs.AddMetric(metric_name, set(), 'sys', sys_scores, replace=True)\n",
    "  evs.AddMetric(metric_name, set(), 'seg', seg_scores, replace=True)\n",
    "\n",
    "\n",
    "for evs in evs_dict.values():\n",
    "  evs.SetPrimaryMetrics(evs.primary_metrics | {metric_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mHtzjRQgXcs2"
   },
   "outputs": [],
   "source": [
    "# @title Generate results with new metric\n",
    "\n",
    "wmt23_tasks, wts = tasks.WMT23(wmt23_lps, k=0)\n",
    "\n",
    "# Takes about 3 minutes.\n",
    "new_results = wmt23_tasks.Run(eval_set_dict=evs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6FDMKPU4d97V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang:                                  en-de,he-en,zh-en     en-de     en-de     en-de     he-en     he-en     he-en     zh-en     zh-en     zh-en\n",
      "level:                                               sys       sys       seg       seg       sys       seg       seg       sys       seg       seg\n",
      "corr_fcn:                                       accuracy   pearson   pearson     acc-t   pearson   pearson     acc-t   pearson   pearson     acc-t\n",
      "metric                       avg-corr              task1     task2     task3     task4     task5     task6     task7     task8     task9    task10\n",
      "---------------------------  --------  -----------------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "XCOMET-Ensemble               1 0.825            3 0.928   4 0.980   1 0.695   1 0.604   6 0.950   1 0.556   1 0.586  10 0.927   1 0.650   1 0.543\n",
      "XCOMET-QE-Ensemble[noref]     2 0.808            6 0.908   9 0.974   2 0.679   4 0.588  15 0.909   4 0.498   4 0.554  16 0.892   3 0.647   3 0.533\n",
      "MetricX-23                    3 0.808            5 0.908   7 0.977   4 0.585   2 0.603  14 0.910   2 0.548   2 0.577  19 0.873   4 0.625   4 0.531\n",
      "Ours[noref]                   4 0.805            2 0.936   5 0.979   7 0.552  13 0.555   1 0.972   7 0.425   9 0.538   2 0.987   5 0.577   8 0.517\n",
      "MetricX-23-QE[noref]          5 0.800           14 0.892  13 0.969   3 0.626   3 0.596  25 0.858   3 0.520   3 0.564  21 0.859   2 0.647   5 0.527\n",
      "mbr-metricx-qe[noref]         6 0.788           20 0.880   8 0.976   5 0.571   5 0.584  12 0.915   8 0.411   5 0.553   8 0.936   9 0.489   2 0.537\n",
      "GEMBA-MQM[noref]              7 0.786            1 0.936  11 0.973  18 0.429  31 0.474   4 0.956   6 0.438   7 0.546   3 0.986  11 0.475  29 0.472\n",
      "MaTESe                        8 0.782            9 0.904  27 0.918   6 0.554  19 0.528  17 0.906   5 0.459   6 0.550  17 0.889   8 0.511  24 0.479\n",
      "_CometKiwi[noref]             9 0.782            8 0.904  18 0.946  12 0.475   9 0.569  24 0.860  13 0.387   8 0.544   5 0.963  13 0.442   6 0.525\n",
      "_COMET                       10 0.779           11 0.900   2 0.990  17 0.432   7 0.574   7 0.940   9 0.401  11 0.532  15 0.898  15 0.396  11 0.514\n",
      "_BLEURT-20                   11 0.776           13 0.892   3 0.990  11 0.484   8 0.572   8 0.937  16 0.382  16 0.519  18 0.880  18 0.378   7 0.518\n",
      "KG-BERTScore[noref]          12 0.774           18 0.884  21 0.926  13 0.451  12 0.556  16 0.908  15 0.382  10 0.537   6 0.962  14 0.430   9 0.516\n",
      "sescoreX                     13 0.772           15 0.892  17 0.952   9 0.519  10 0.563  19 0.901  14 0.385  24 0.484  25 0.797   6 0.536  14 0.499\n",
      "cometoid22-wmt22[noref]      14 0.772           19 0.880  10 0.973  15 0.441   6 0.578  28 0.839  19 0.365  17 0.515   7 0.940  10 0.479  10 0.515\n",
      "_docWMT22CometDA             15 0.768           10 0.904   1 0.990  21 0.394  11 0.559  10 0.922  20 0.339  22 0.497  11 0.907  20 0.353  18 0.493\n",
      "_docWMT22CometKiwiDA[noref]  16 0.767           12 0.900  12 0.970  14 0.444  14 0.547  18 0.906  25 0.286  23 0.489   4 0.965  17 0.387  17 0.493\n",
      "Calibri-COMET22              17 0.767            7 0.904  14 0.963  20 0.413  23 0.522   9 0.930  10 0.401  18 0.515  20 0.863  16 0.396  27 0.474\n",
      "EAPrompt[noref]              18 0.757           17 0.884  15 0.962   8 0.520  32 0.471  27 0.840  18 0.374  28 0.450  12 0.907   7 0.516  32 0.452\n",
      "Calibri-COMET22-QE[noref]    19 0.755           24 0.863   6 0.978  16 0.441  30 0.483  30 0.778  12 0.395  20 0.506   9 0.934  12 0.443  19 0.491\n",
      "_YiSi-1                      20 0.754           23 0.871  22 0.925  22 0.366  16 0.542  11 0.917  11 0.395  12 0.529  22 0.823  22 0.290  12 0.504\n",
      "_MS-COMET-QE-22[noref]       21 0.744           22 0.871  16 0.959  24 0.310  15 0.546  33 0.721  24 0.295  21 0.498  14 0.901  19 0.367  16 0.498\n",
      "_prismRef                    22 0.744           27 0.851  24 0.920  10 0.516  27 0.518   3 0.956  22 0.319  13 0.528  30 0.762  25 0.183  13 0.504\n",
      "mre-score-labse-regular      23 0.743           16 0.888  19 0.942  32 0.111  17 0.530   2 0.958  17 0.378  15 0.522  13 0.903  27 0.145  22 0.481\n",
      "_BERTscore                   24 0.742           21 0.871  29 0.891  23 0.325  20 0.528  20 0.895  21 0.335  19 0.515  23 0.810  23 0.236  15 0.499\n",
      "GEMBA-DA[noref]              25 0.733            4 0.912  20 0.932  35-0.175  34 0.412   5 0.951  23 0.315  14 0.525   1 0.990  21 0.296  26 0.474\n",
      "XLsim                        26 0.719           26 0.855  23 0.925  26 0.239  21 0.527  21 0.887  27 0.233  25 0.480  26 0.796  29 0.111  31 0.464\n",
      "_f200spBLEU                  27 0.704           30 0.819  25 0.919  27 0.237  22 0.526  29 0.805  28 0.230  29 0.447  29 0.772  30 0.108  25 0.476\n",
      "MEE4                         28 0.704           29 0.823  31 0.861  30 0.202  18 0.529  22 0.879  26 0.256  33 0.441  31 0.743  31 0.105  23 0.480\n",
      "tokengram_F                  29 0.703           32 0.815  33 0.858  29 0.227  24 0.520  23 0.878  29 0.226  26 0.461  27 0.795  33 0.060  21 0.485\n",
      "embed_llama                  30 0.701           28 0.831  32 0.861  25 0.250  29 0.483  26 0.841  32 0.215  34 0.430  28 0.785  26 0.161  33 0.447\n",
      "_BLEU                        31 0.696           31 0.815  28 0.917  31 0.192  25 0.520  32 0.769  31 0.220  31 0.442  32 0.734  28 0.119  30 0.472\n",
      "_chrF                        32 0.694           33 0.795  30 0.866  28 0.232  26 0.519  31 0.776  30 0.221  27 0.460  24 0.809  32 0.063  20 0.485\n",
      "eBLEU                        33 0.692           25 0.859  26 0.918  34-0.011  28 0.512  13 0.911  34 0.131  30 0.445  33 0.727  35-0.084  28 0.473\n",
      "_Random-sysname[noref]       34 0.529           34 0.578  34 0.357  33 0.064  35 0.409  34 0.209  35 0.041  35 0.428  34 0.093  34 0.018  35 0.381\n",
      "_prismSrc[noref]             35 0.455           35 0.386  35-0.327  19 0.425  33 0.426  35-0.017  33 0.140  32 0.441  35-0.406  24 0.223  34 0.421\n",
      "\n",
      "{'XCOMET-Ensemble': 0.8245564693261692, 'XCOMET-QE-Ensemble[noref]': 0.8081704576276468, 'MetricX-23': 0.8077332042121148, 'Ours[noref]': 0.8053447959344475, 'MetricX-23-QE[noref]': 0.8001022435243909, 'mbr-metricx-qe[noref]': 0.7884462134687077, 'GEMBA-MQM[noref]': 0.7856482538527599, 'MaTESe': 0.7822095186206205, 'CometKiwi[noref]': 0.7820590225603948, 'COMET': 0.77889912258094, 'BLEURT-20': 0.7758050129307184, 'KG-BERTScore[noref]': 0.7741843374057716, 'sescoreX': 0.7722126986983358, 'cometoid22-wmt22[noref]': 0.7720787682552798, 'docWMT22CometDA': 0.7676496183978226, 'docWMT22CometKiwiDA[noref]': 0.7671523448583066, 'Calibri-COMET22': 0.7670585057405693, 'EAPrompt[noref]': 0.7569187393685334, 'Calibri-COMET22-QE[noref]': 0.7545793620707139, 'YiSi-1': 0.7540730155751062, 'MS-COMET-QE-22[noref]': 0.7443669261453028, 'prismRef': 0.7443414532589709, 'mre-score-labse-regular': 0.7428204439902026, 'BERTscore': 0.7418818840681006, 'GEMBA-DA[noref]': 0.7333872287879148, 'XLsim': 0.7194070335642098, 'f200spBLEU': 0.7036025944759181, 'MEE4': 0.703550220830354, 'tokengram_F': 0.7028138318101136, 'embed_llama': 0.7009574974233395, 'BLEU': 0.6961723674714032, 'chrF': 0.6944183794004583, 'eBLEU': 0.6920469428468609, 'Random-sysname[noref]': 0.5286910305258041, 'prismSrc[noref]': 0.45522501232336293}\n"
     ]
    }
   ],
   "source": [
    "# @title Print results\n",
    "\n",
    "avg_corrs = new_results.AverageCorrs(wts)\n",
    "\n",
    "table = new_results.Table(\n",
    "    metrics=list(avg_corrs),\n",
    "    initial_column=avg_corrs,\n",
    "    initial_column_header='avg-corr',\n",
    "    attr_list=['lang', 'level', 'corr_fcn'],\n",
    "    nicknames={'KendallWithTiesOpt': 'acc-t'},\n",
    "    fmt='text',\n",
    "    baselines_metainfo=meta_info.WMT23)\n",
    "\n",
    "print(table)\n",
    "print(avg_corrs)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1UgUZ35EdmwwuDljJMtlz5vAaOT4blX8J",
     "timestamp": 1699484321090
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
